{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "\n",
    "import geopandas as gpd\n",
    "import shapely.geometry\n",
    "import rasterio\n",
    "import json\n",
    "import geopandas as gpd\n",
    "import geopandas_osm.osm\n",
    "from descartes import PolygonPatch\n",
    "import h5py \n",
    "from scipy.misc import imresize\n",
    "import shapely.geometry\n",
    "\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import pickle\n",
    "import copy\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tile_no</th>\n",
       "      <th>flooded</th>\n",
       "      <th>post_resized</th>\n",
       "      <th>pre_resized</th>\n",
       "      <th>post_full</th>\n",
       "      <th>pre_full</th>\n",
       "      <th>course_mask_full</th>\n",
       "      <th>course_mask_resized</th>\n",
       "      <th>fine_mask_filename</th>\n",
       "      <th>footprint</th>\n",
       "      <th>dry_or_wet</th>\n",
       "      <th>mask_poly</th>\n",
       "      <th>tile_transform</th>\n",
       "      <th>geometry</th>\n",
       "      <th>DBScan</th>\n",
       "      <th>DBScan_gauss</th>\n",
       "      <th>bad_image</th>\n",
       "      <th>problem</th>\n",
       "      <th>verified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.80323</td>\n",
       "      <td>0_post_resize_img</td>\n",
       "      <td>0_pre_resize_img</td>\n",
       "      <td>0_post_full_img</td>\n",
       "      <td>0_pre_full_img</td>\n",
       "      <td>0_mask</td>\n",
       "      <td>0_resize_mask</td>\n",
       "      <td>0_256_fine_mask</td>\n",
       "      <td>3002220.tif</td>\n",
       "      <td>wet</td>\n",
       "      <td>(POLYGON ((-95.57181511210993 29.4410615808823...</td>\n",
       "      <td>[222822.4, 0.0, 0.0, -222822.4, 21295616.0, 65...</td>\n",
       "      <td>POLYGON ((-95.56985294117646 29.44106158088235...</td>\n",
       "      <td>0_256_DBSCAN</td>\n",
       "      <td>0_256_fine_mask_blur</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.8787</td>\n",
       "      <td>1_post_resize_img</td>\n",
       "      <td>1_pre_resize_img</td>\n",
       "      <td>1_post_full_img</td>\n",
       "      <td>1_pre_full_img</td>\n",
       "      <td>1_mask</td>\n",
       "      <td>1_resize_mask</td>\n",
       "      <td>1_256_fine_mask</td>\n",
       "      <td>3002220.tif</td>\n",
       "      <td>wet</td>\n",
       "      <td>POLYGON ((-95.56764981800494 29.44136752727207...</td>\n",
       "      <td>[222822.4, 0.0, 0.0, -222822.4, 21295104.0, 65...</td>\n",
       "      <td>POLYGON ((-95.56755514705883 29.44106158088235...</td>\n",
       "      <td>1_256_DBSCAN</td>\n",
       "      <td>1_256_fine_mask_blur</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  tile_no  flooded       post_resized       pre_resized        post_full  \\\n",
       "0       0  0.80323  0_post_resize_img  0_pre_resize_img  0_post_full_img   \n",
       "1       1   0.8787  1_post_resize_img  1_pre_resize_img  1_post_full_img   \n",
       "\n",
       "         pre_full course_mask_full course_mask_resized fine_mask_filename  \\\n",
       "0  0_pre_full_img           0_mask       0_resize_mask    0_256_fine_mask   \n",
       "1  1_pre_full_img           1_mask       1_resize_mask    1_256_fine_mask   \n",
       "\n",
       "     footprint dry_or_wet                                          mask_poly  \\\n",
       "0  3002220.tif        wet  (POLYGON ((-95.57181511210993 29.4410615808823...   \n",
       "1  3002220.tif        wet  POLYGON ((-95.56764981800494 29.44136752727207...   \n",
       "\n",
       "                                      tile_transform  \\\n",
       "0  [222822.4, 0.0, 0.0, -222822.4, 21295616.0, 65...   \n",
       "1  [222822.4, 0.0, 0.0, -222822.4, 21295104.0, 65...   \n",
       "\n",
       "                                            geometry        DBScan  \\\n",
       "0  POLYGON ((-95.56985294117646 29.44106158088235...  0_256_DBSCAN   \n",
       "1  POLYGON ((-95.56755514705883 29.44106158088235...  1_256_DBSCAN   \n",
       "\n",
       "           DBScan_gauss bad_image problem verified  \n",
       "0  0_256_fine_mask_blur      None    None    False  \n",
       "1  1_256_fine_mask_blur      None    None     True  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir = \"/home/ubuntu/data/TX_paired/\"\n",
    "\n",
    "geo_df = pickle.load( open( output_dir+\"GeoDataFrame_fine_turked.pickled\", \"rb\" ))\n",
    "geo_df.rename(columns = {'post-storm_full':'post_resized','pre-storm_full':'pre_resized','post-storm_resized':'post_full','pre-storm_resized':'pre_full'}, inplace=True)\n",
    "geo_df.set_index(\"tile_no\")\n",
    "geo_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1271"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#only use 'good' and verified files\n",
    "good_geo_df = geo_df[geo_df.bad_image != True]\n",
    "good_geo_df = good_geo_df[good_geo_df.verified == True]\n",
    "len(good_geo_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tile_no                                                              273\n",
       "flooded                                                       0.00862122\n",
       "post_resized                                         273_post_resize_img\n",
       "pre_resized                                           273_pre_resize_img\n",
       "post_full                                              273_post_full_img\n",
       "pre_full                                                273_pre_full_img\n",
       "course_mask_full                                                273_mask\n",
       "course_mask_resized                                      273_resize_mask\n",
       "fine_mask_filename                                     273_256_fine_mask\n",
       "footprint                                                    3020010.tif\n",
       "dry_or_wet                                                           wet\n",
       "mask_poly              POLYGON ((-95.41973723733823 29.57318474264706...\n",
       "tile_transform         [222822.4, 0.0, 0.0, -222822.4, 21262080.0, 65...\n",
       "geometry               POLYGON ((-95.41934742647059 29.57318474264706...\n",
       "DBScan                                                    273_256_DBSCAN\n",
       "DBScan_gauss                                      273_256_fine_mask_blur\n",
       "bad_image                                                           None\n",
       "problem                                                             None\n",
       "verified                                                            True\n",
       "Name: 273, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_geo_df.loc[273]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "INPUT_SIZE = 256\n",
    "\n",
    "#helper functions:\n",
    "def scale_bands(img, lower_pct = 1, upper_pct = 99):\n",
    "    \"\"\"Rescale the bands of a multichannel image for display\"\"\"\n",
    "    img_scaled = np.zeros(img.shape, np.uint8)\n",
    "    for i in range(img.shape[2]):\n",
    "        band = img[:, :, i]\n",
    "        lower, upper = np.percentile(band, [lower_pct, upper_pct])\n",
    "        band = (band - lower) / (upper - lower) * 255\n",
    "        img_scaled[:, :, i] = np.clip(band, 0, 255).astype(np.uint8)\n",
    "    return img_scaled\n",
    "\n",
    "def resize(image, new_shape):\n",
    "    #img_resized = np.zeros(new_shape+(img.shape[2],)).astype('float32')\n",
    "    #for i in range(img.shape[2]):\n",
    "    #    img_resized[:, :, i] = imresize(img[:, :, i], new_shape, interp='bicubic')\n",
    "    #img_resized = cv2.resize(img,dsize=(new_shape,new_shape))\n",
    "    \n",
    "    r = new_shape / (1.0*image.shape[1])\n",
    "    dim = (new_shape, int(image.shape[0] * r))\n",
    "    \n",
    "    if image.dtype == \"int64\": image = image.astype('float32')   #crashes if given 0 and 1 integers for some reason\n",
    "    # perform the actual resizing of the image and show it\n",
    "    img_resized = cv2.resize(image, dim, interpolation = cv2.INTER_AREA)\n",
    "    \n",
    "    #make training masks the proper dimensionality if only 2-D\n",
    "    if len(image.shape) == 2: img_resized = np.expand_dims(img_resized,axis=2)\n",
    "\n",
    "    return img_resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### re-load the a training/testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test/train split already done, load from files\n",
    "#assumes image and mask files are of identical number and none are missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svg_test_tile_no.txt\r\n",
      "svg_train_tile_no.txt\r\n",
      "test_images.txt\r\n",
      "test_masks.txt\r\n",
      "test_tile_no.txt\r\n",
      "train_images.txt\r\n",
      "train_masks.txt\r\n",
      "train_tile_no.txt\r\n",
      "Unn_test_tile_no.txt\r\n",
      "Unn_test_tile_numbers2017-09-17 07:25:33.050249.txt\r\n",
      "Unn_test_tile_numbers_2017-09-17 07:29:04.079126.txt\r\n",
      "Unn_test_tile_numbers_2017-09-17 07:41:00.060471.txt\r\n",
      "Unn_test_tile_numbers_2017-09-17 19:37:41.158566.txt\r\n",
      "Unn_test_tile_numbers_2017-09-17 22:30:50.698961.txt\r\n",
      "Unn_test_tile_numbers_2017-09-17 22:45:12.936752.txt\r\n",
      "Unn_test_tile_numbers_2017-09-17 22:48:07.272803.txt\r\n",
      "Unn_test_tile_numbers_2017-09-18 00:19:49.353153.txt\r\n",
      "Unn_test_tile_numbers_2017-09-18 01:05:02.180177.txt\r\n",
      "Unn_test_tile_numbers_2017-09-18 01:30:05.206556.txt\r\n",
      "Unn_test_tile_numbers_2017-09-18 01:47:59.734463.txt\r\n",
      "Unn_test_tile_numbers_2017-09-18 02:16:12.497202.txt\r\n",
      "Unn_test_tile_numbers_2017-09-18 16:19:42.641046.txt\r\n",
      "Unn_test_tile_numbers_2017-09-18 16:22:02.923366.txt\r\n",
      "Unn_test_tile_numbers_2017-09-18 23:12:21.055002.txt\r\n",
      "Unn_test_tile_numbers_2017-09-18 23:16:51.606428.txt\r\n",
      "Unn_train_tile_no.txt\r\n",
      "Unn_train_tile_numbers2017-09-17 07:25:33.049349.txt\r\n",
      "Unn_train_tile_numbers_2017-09-17 07:29:04.078846.txt\r\n",
      "Unn_train_tile_numbers_2017-09-17 07:41:00.060192.txt\r\n",
      "Unn_train_tile_numbers_2017-09-17 19:37:41.157277.txt\r\n",
      "Unn_train_tile_numbers_2017-09-17 22:30:50.698311.txt\r\n",
      "Unn_train_tile_numbers_2017-09-17 22:45:12.935961.txt\r\n",
      "Unn_train_tile_numbers_2017-09-17 22:48:07.272250.txt\r\n",
      "Unn_train_tile_numbers_2017-09-18 00:19:49.352305.txt\r\n",
      "Unn_train_tile_numbers_2017-09-18 01:05:02.176213.txt\r\n",
      "Unn_train_tile_numbers_2017-09-18 01:30:05.205929.txt\r\n",
      "Unn_train_tile_numbers_2017-09-18 01:47:59.730284.txt\r\n",
      "Unn_train_tile_numbers_2017-09-18 02:16:12.496553.txt\r\n",
      "Unn_train_tile_numbers_2017-09-18 16:19:42.639276.txt\r\n",
      "Unn_train_tile_numbers_2017-09-18 16:22:02.922903.txt\r\n",
      "Unn_train_tile_numbers_2017-09-18 23:12:21.053673.txt\r\n",
      "Unn_train_tile_numbers_2017-09-18 23:16:51.605604.txt\r\n",
      "xgb_test_tile_no_2017-09-18 00:22:55.570603.txt\r\n",
      "xgb_test_tile_no_2017-09-18 01:11:25.543291.txt\r\n",
      "xgb_test_tile_no_2017-09-18 01:18:54.829559.txt\r\n",
      "xgb_test_tile_no_2017-09-18 16:23:02.720797.txt\r\n",
      "xgb_test_tile_no_2017-09-18 23:22:05.934497.txt\r\n",
      "xgb_test_tile_no_2017-09-18 23:26:59.047225.txt\r\n",
      "xgb_test_tile_no_2017-09-18 23:28:34.438047.txt\r\n",
      "xgb_test_tile_no_2017-09-18 23:33:36.608345.txt\r\n",
      "xgb_test_tile_no_2017-09-18 23:45:30.579152.txt\r\n",
      "xgb_test_tile_no_2017-09-19 00:09:36.592403.txt\r\n",
      "xgb_test_tile_no_2017-09-19 00:09:52.202720.txt\r\n",
      "xgb_test_tile_no.txt\r\n",
      "xgb_train_tile_no_2017-09-18 00:22:55.569857.txt\r\n",
      "xgb_train_tile_no_2017-09-18 01:11:25.542443.txt\r\n",
      "xgb_train_tile_no_2017-09-18 01:18:54.828552.txt\r\n",
      "xgb_train_tile_no_2017-09-18 16:23:02.719797.txt\r\n",
      "xgb_train_tile_no_2017-09-18 23:22:05.933537.txt\r\n",
      "xgb_train_tile_no_2017-09-18 23:26:59.046363.txt\r\n",
      "xgb_train_tile_no_2017-09-18 23:28:34.437209.txt\r\n",
      "xgb_train_tile_no_2017-09-18 23:33:36.607944.txt\r\n",
      "xgb_train_tile_no_2017-09-18 23:45:30.578556.txt\r\n",
      "xgb_train_tile_no_2017-09-19 00:09:36.591901.txt\r\n",
      "xgb_train_tile_no_2017-09-19 00:09:52.202290.txt\r\n",
      "xgb_train_tile_no.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls /home/ubuntu/Notebooks/test_train_filelists/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('/home/ubuntu/Notebooks/test_train_filelists/Unn_test_tile_numbers_2017-09-18 16:22:02.923366.txt', 'r')\n",
    "test_no = f.readlines()\n",
    "test_no = [file.strip(\"\\n\") for file in test_no]\n",
    "test_no = [int(n) for n in test_no]\n",
    "\n",
    "f = open('/home/ubuntu/Notebooks/test_train_filelists/Unn_train_tile_numbers_2017-09-18 16:22:02.922903.txt', 'r')\n",
    "train_no = f.readlines()\n",
    "train_no = [file.strip(\"\\n\") for file in train_no]\n",
    "train_no = [int(n) for n in train_no]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1046, 256)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_no),len(test_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-MAKE THE TESTING AND TRAINGING DATA SETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_train(good_geo_df,train_no):\n",
    "    indexes = copy.deepcopy(train_no)\n",
    "    while True:\n",
    "        np.random.shuffle(indexes)\n",
    "\n",
    "        for index in indexes:\n",
    "            Xpost = np.load(output_dir+good_geo_df.post_resized[index]+'.npy')\n",
    "            Xpre = np.load(output_dir+good_geo_df.pre_resized[index]+'.npy')\n",
    "            Xpre = Xpre.astype('float32')\n",
    "            Xpost = Xpost.astype('float32')\n",
    "            pre_mean = 92.36813   #taken from a central common image\n",
    "            post_mean = 92.21524   #much closer than expected... are these representative?\n",
    "            \n",
    "            Xdiff = Xpost/post_mean - Xpre/pre_mean\n",
    "            \n",
    "            Xpost = (Xpost-post_mean)/post_mean  #divide by their respective means (per footprint would be even better)\n",
    "            Xpre =  (Xpre-pre_mean)/pre_mean\n",
    "            \n",
    "            R,G,B = Xpost[:,:,0],Xpost[:,:,1],Xpost[:,:,2]\n",
    "            Xratios_post = np.stack([R/G-1,R/B-1,G/B-1,R/(G+B)-0.5,G/(R+B)-0.5,B/(R+G)-0.5],axis=2)\n",
    "            \n",
    "            R,G,B = Xpre[:,:,0],Xpre[:,:,1],Xpre[:,:,2]\n",
    "            Xratios_pre = np.stack([R/G-1,R/B-1,G/B-1,R/(G+B)-0.5,G/(R+B)-0.5,B/(R+G)-0.5],axis=2)\n",
    "            \n",
    "            #X = np.concatenate([Xpost,Xdiff,Xpre,Xratios_post,Xratios_pre],axis=2)\n",
    "            X = np.concatenate([Xpost-1,Xdiff-1,Xratios_post],axis=2)\n",
    "             \n",
    "            Y = np.load(output_dir+good_geo_df.fine_mask_filename[index]+'.npy')\n",
    "            Y = Y.astype('float32') #/ 255.\n",
    "            #add extra first dimension for tensorflow compatability\n",
    "            X = np.expand_dims(X,axis=0)\n",
    "            Y = np.expand_dims(Y,axis=0)\n",
    "            Y = np.expand_dims(Y,axis=3)\n",
    "            yield (X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_test(good_geo_df,test_no):\n",
    "    indexes = copy.deepcopy(test_no)\n",
    "    while True:\n",
    "        np.random.shuffle(indexes)\n",
    "\n",
    "        for index in indexes:\n",
    "            Xpost = np.load(output_dir+good_geo_df.post_resized[index]+'.npy')\n",
    "            Xpre = np.load(output_dir+good_geo_df.pre_resized[index]+'.npy')\n",
    "            Xpre = Xpre.astype('float32')\n",
    "            Xpost = Xpost.astype('float32')\n",
    "            pre_mean = 92.36813   #taken from a central common image\n",
    "            post_mean = 92.21524   #much closer than expected... are these representative?\n",
    "            \n",
    "            Xdiff = Xpost/post_mean - Xpre/pre_mean\n",
    "            \n",
    "            Xpost = (Xpost-post_mean)/post_mean  #divide by their respective means (per footprint would be even better)\n",
    "            Xpre =  (Xpre-pre_mean)/pre_mean\n",
    "            \n",
    "            R,G,B = Xpost[:,:,0],Xpost[:,:,1],Xpost[:,:,2]\n",
    "            Xratios_post = np.stack([R/G-1,R/B-1,G/B-1,R/(G+B)-0.5,G/(R+B)-0.5,B/(R+G)-0.5],axis=2)\n",
    "            \n",
    "            R,G,B = Xpre[:,:,0],Xpre[:,:,1],Xpre[:,:,2]\n",
    "            Xratios_pre = np.stack([R/G-1,R/B-1,G/B-1,R/(G+B)-0.5,G/(R+B)-0.5,B/(R+G)-0.5],axis=2)\n",
    "            \n",
    "            #X = np.concatenate([Xpost,Xdiff,Xpre,Xratios_post,Xratios_pre],axis=2)\n",
    "            X = np.concatenate([Xpost-1,Xdiff-1,Xratios_post],axis=2)             \n",
    "            Y = np.load(output_dir+good_geo_df.fine_mask_filename[index]+'.npy')\n",
    "            Y = Y.astype('float32') #/ 255.\n",
    "            #add extra first dimension for tensorflow compatability\n",
    "            X = np.expand_dims(X,axis=0)\n",
    "            Y = np.expand_dims(Y,axis=0)\n",
    "            Y = np.expand_dims(Y,axis=3)\n",
    "            yield (X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keep in memory versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train(good_geo_df,train_no):\n",
    "    indexes = copy.deepcopy(train_no)\n",
    "    \n",
    "    np.random.shuffle(indexes)\n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "    \n",
    "    for index in indexes:\n",
    "        Xpost = np.load(output_dir+good_geo_df.post_resized[index]+'.npy')\n",
    "        Xpre = np.load(output_dir+good_geo_df.pre_resized[index]+'.npy')\n",
    "        Xpre = Xpre.astype('float32')\n",
    "        Xpost = Xpost.astype('float32')\n",
    "        pre_mean = 92.36813   #taken from a central common image\n",
    "        post_mean = 92.21524   #much closer than expected... are these representative?\n",
    "\n",
    "        Xdiff = Xpost/post_mean - Xpre/pre_mean\n",
    "\n",
    "        Xpost = (Xpost-post_mean)/post_mean  #divide by their respective means (per footprint would be even better)\n",
    "        Xpre =  (Xpre-pre_mean)/pre_mean\n",
    "\n",
    "        R,G,B = Xpost[:,:,0],Xpost[:,:,1],Xpost[:,:,2]\n",
    "        Xratios_post = np.stack([R/G-1,R/B-1,G/B-1,R/(G+B)-0.5,G/(R+B)-0.5,B/(R+G)-0.5],axis=2)\n",
    "\n",
    "        R,G,B = Xpre[:,:,0],Xpre[:,:,1],Xpre[:,:,2]\n",
    "        Xratios_pre = np.stack([R/G-1,R/B-1,G/B-1,R/(G+B)-0.5,G/(R+B)-0.5,B/(R+G)-0.5],axis=2)\n",
    "\n",
    "        #X = np.concatenate([Xpost,Xdiff,Xpre,Xratios_post,Xratios_pre],axis=2)\n",
    "        X = np.concatenate([Xpost-1,Xdiff-1,Xratios_post],axis=2)\n",
    "        Y = np.load(output_dir+good_geo_df.fine_mask_filename[index]+'.npy')\n",
    "        Y = Y.astype('float32') #/ 255.\n",
    "        \n",
    "        #X = np.reshape(X,(X.shape[0]**2,X.shape[2]))\n",
    "        #Y = np.reshape(Y,(Y.shape[0]**2))\n",
    "        \n",
    "        X_train.append(X)\n",
    "        Y_train.append(Y)\n",
    "        \n",
    "    X_train = np.asarray(X_train)\n",
    "    Y_train = np.asarray(Y_train)\n",
    "\n",
    "    #X_train = np.reshape(X_train,(X_train.shape[0]*X_train.shape[1],X_train.shape[2]))\n",
    "    #Y_train = np.reshape(Y_train,(Y_train.shape[0]*Y_train.shape[1]))\n",
    "                         \n",
    "    return X_train, Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_test(good_geo_df,test_no):\n",
    "    indexes = copy.deepcopy(test_no)\n",
    "    \n",
    "    np.random.shuffle(indexes)\n",
    "    X_test = []\n",
    "    Y_test = []\n",
    "    \n",
    "    for index in indexes:\n",
    "        Xpost = np.load(output_dir+good_geo_df.post_resized[index]+'.npy')\n",
    "        Xpre = np.load(output_dir+good_geo_df.pre_resized[index]+'.npy')\n",
    "        Xpre = Xpre.astype('float32')\n",
    "        Xpost = Xpost.astype('float32')\n",
    "        pre_mean = 92.36813   #taken from a central common image\n",
    "        post_mean = 92.21524   #much closer than expected... are these representative?\n",
    "\n",
    "        Xdiff = Xpost/post_mean - Xpre/pre_mean\n",
    "\n",
    "        Xpost = (Xpost-post_mean)/post_mean  #divide by their respective means (per footprint would be even better)\n",
    "        Xpre =  (Xpre-pre_mean)/pre_mean\n",
    "\n",
    "        R,G,B = Xpost[:,:,0],Xpost[:,:,1],Xpost[:,:,2]\n",
    "        Xratios_post = np.stack([R/G-1,R/B-1,G/B-1,R/(G+B)-0.5,G/(R+B)-0.5,B/(R+G)-0.5],axis=2)\n",
    "\n",
    "        R,G,B = Xpre[:,:,0],Xpre[:,:,1],Xpre[:,:,2]\n",
    "        Xratios_pre = np.stack([R/G-1,R/B-1,G/B-1,R/(G+B)-0.5,G/(R+B)-0.5,B/(R+G)-0.5],axis=2)\n",
    "\n",
    "        #X = np.concatenate([Xpost,Xdiff,Xpre,Xratios_post,Xratios_pre],axis=2)\n",
    "        X = np.concatenate([Xpost-1,Xdiff-1,Xratios_post],axis=2)\n",
    "        Y = np.load(output_dir+good_geo_df.fine_mask_filename[index]+'.npy')\n",
    "        Y = Y.astype('float32') #/ 255.\n",
    "        \n",
    "        #X = np.reshape(X,(X.shape[0]**2,X.shape[2]))\n",
    "        #Y = np.reshape(Y,(Y.shape[0]**2))\n",
    "        \n",
    "        X_test.append(X)\n",
    "        Y_test.append(Y)\n",
    "        \n",
    "    X_test = np.asarray(X_test)\n",
    "    Y_test = np.asarray(Y_test)\n",
    "\n",
    "    #X_test = np.reshape(X_test,(X_test.shape[0]*X_test.shape[1],X_test.shape[2]))\n",
    "    #Y_test = np.reshape(Y_test,(Y_test.shape[0]*Y_test.shape[1]))\n",
    "                         \n",
    "    return X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train,Y_train = get_train(good_geo_df,train_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'850_post_resize_img'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_geo_df.post_resized[850]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "1795",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-da6479b3665c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mget_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgood_geo_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-6e95d1fd1604>\u001b[0m in \u001b[0;36mget_test\u001b[0;34m(good_geo_df, test_no)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindexes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mXpost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mgood_geo_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_resized\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mXpre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mgood_geo_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_resized\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mXpre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXpre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pandas/core/series.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pandas/core/indexes/base.pyc\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   2475\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2476\u001b[0m             return self._engine.get_value(s, k,\n\u001b[0;32m-> 2477\u001b[0;31m                                           tz=getattr(series.dtype, 'tz', None))\n\u001b[0m\u001b[1;32m   2478\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2479\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minferred_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'integer'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'boolean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value (pandas/_libs/index.c:4404)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value (pandas/_libs/index.c:4087)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc (pandas/_libs/index.c:5126)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item (pandas/_libs/hashtable.c:14031)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item (pandas/_libs/hashtable.c:13975)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 1795"
     ]
    }
   ],
   "source": [
    "X_test, Y_test  = get_test(good_geo_df,test_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://github.com/jocicmarko/ultrasound-nerve-segmentation/blob/master/train.py\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, concatenate, Conv2D, MaxPooling2D, Conv2DTranspose, Dropout, BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam, Nadam, Adamax, RMSprop\n",
    "\n",
    "# Set network size params\n",
    "N_CLASSES = 1\n",
    "N_CHANNEL = 12\n",
    "\n",
    "#dropout rate\n",
    "out_per = 0.30\n",
    "\n",
    "# Define metrics\n",
    "smooth = 1.\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "# Just put a negative sign in front of an accuracy metric to turn it into a loss to be minimized\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return -dice_coef(y_true, y_pred)\n",
    "\n",
    "def jacc_coef(y_true, y_pred):\n",
    "    intersection = K.sum(y_true * y_pred, axis=[0, -1, -2])\n",
    "    sum_ = K.sum(y_true + y_pred, axis=[0, -1, -2])\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "    return K.mean(jac)\n",
    "\n",
    "def jacc_coef_loss(y_true, y_pred):\n",
    "    return -jacc_coef(y_true, y_pred)\n",
    "\n",
    "def jacc_coef_int(y_true, y_pred):\n",
    "    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
    "    intersection = K.sum(y_true * y_pred_pos, axis=[0, -1, -2])\n",
    "    sum_ = K.sum(y_true + y_pred, axis=[0, -1, -2])\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "    return K.mean(jac)\n",
    "\n",
    "def get_unet(lr=1e-5):\n",
    "    inputs = Input((INPUT_SIZE, INPUT_SIZE, N_CHANNEL))\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(inputs)\n",
    "    conv1 = Dropout(out_per)(conv1)\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(pool1)\n",
    "    conv2 = Dropout(out_per)(conv2)\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(pool2)\n",
    "    conv3 = Dropout(out_per)(conv3)\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(pool3)\n",
    "    conv4 = Dropout(out_per)(conv4)\n",
    "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(pool4)\n",
    "    conv5 = Dropout(out_per)(conv5)\n",
    "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv5)\n",
    "\n",
    "    up6 = concatenate([Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv5), conv4], axis=3)\n",
    "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(up6)\n",
    "    conv6 = Dropout(out_per)(conv6)\n",
    "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv6)\n",
    "\n",
    "    up7 = concatenate([Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv6), conv3], axis=3)\n",
    "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(up7)\n",
    "    conv7 = Dropout(out_per)(conv7)\n",
    "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv7)\n",
    "\n",
    "    up8 = concatenate([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv7), conv2], axis=3)\n",
    "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(up8)\n",
    "    conv8 = Dropout(out_per)(conv8)\n",
    "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv8)\n",
    "\n",
    "    up9 = concatenate([Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv8), conv1], axis=3)\n",
    "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(up9)\n",
    "    conv9 = Dropout(out_per)(conv9)\n",
    "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv9)\n",
    "    conv9 = BatchNormalization(axis = 1)(conv9)\n",
    "    \n",
    "    conv10 = Conv2D(N_CLASSES, (1, 1), activation='sigmoid')(conv9)\n",
    "\n",
    "    model = Model(inputs=[inputs], outputs=[conv10])\n",
    "\n",
    "    # model.compile(optimizer=Adam(lr=lr), loss=jacc_coef_loss, metrics=[jacc_coef_int])\n",
    "    # model.compile(optimizer=Adam(lr=lr), loss='binary_crossentropy', metrics=[jacc_coef_int])\n",
    "    # model.compile(optimizer=Adam(lr=lr), loss='binary_crossentropy', metrics=[dice_coef])\n",
    "    model.compile(optimizer=Adam(lr=lr), loss=dice_coef_loss, metrics=[dice_coef, jacc_coef_int])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View some output\n",
    "Now I'll test out the model by testing it on a single image and looking at the ground truth against the model prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to create an iterator with examples\n",
    "def get_examples():\n",
    "    for i in range(Y_test.shape[0]):\n",
    "        if True: #Y_test[i, ...].sum() >= 3000:  #I want to use this to look at all now\n",
    "            X = X_test[i]\n",
    "            Y = Y_test[i]\n",
    "            yield X, Y, i\n",
    "\n",
    "# Function to plot a single example\n",
    "def plot_example_class(X, Y, model,n_classes=1):\n",
    "    # Apply the model to this example\n",
    "    print(\"predicting...\")\n",
    "    prediction = model.predict(X[None, ...])[0, ...] > 0.15 #was 0.15\n",
    "    print(prediction.mean())\n",
    "    \n",
    "    fig, axes = plt.subplots(n_classes,3, figsize=(18,8))\n",
    "    \n",
    "    # Iterate through each target class\n",
    "    print(Y.shape)\n",
    "    for tno in range(n_classes):\n",
    "        print(tno)\n",
    "        targ = Y[:, :, tno]\n",
    "        pred = prediction[:, :, tno]\n",
    "\n",
    "        if n_classes == 1: ax1, ax2, ax3 = axes[:] #axes[tno,:]\n",
    "        else: ax1, ax2, ax3 = axes[tno,:]\n",
    "\n",
    "        #ax1.imshow(scale_bands(X[:, :, [4,2,1]])) # This index starts at 0, so I had to decrement\n",
    "        ax1.imshow(X[:,:,0:3])\n",
    "        ax2.imshow(targ, vmin=0, vmax=1)\n",
    "        ax3.imshow(pred, vmin=0, vmax=1)\n",
    "\n",
    "        ax1.set_title('Image')\n",
    "        ax2.set_title('Ground Truth')\n",
    "        ax3.set_title('Prediction')\n",
    "    plt.show()\n",
    "\n",
    "example_generator = get_examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # If you need to load the model, you can use these lines.  Change the weights filename to match one you have.\n",
    "model = get_unet(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('checkpoints/weights.43--0.52986.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check out model accuracy: \n",
    "acc_list = []\n",
    "example_generator2 = get_examples()\n",
    "for i in range(len(X_test)-2):\n",
    "    X_ex, Y_ex, index = next(example_generator2)\n",
    "    pred = model.predict(X_ex[None, ...]) > 0.98\n",
    "    accuracy = (pred[0,:,:,0]==Y_ex).sum()*1.0/pred.size\n",
    "    acc_list.append(accuracy)\n",
    "len(acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(acc_list)/len(acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision/Recall Curve and AUC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#example_generator3 = get_examples()\n",
    "#X_ex, Y_ex,index  = next(example_generator3)\n",
    "X_test, Y_test, index  = get_test(good_geo_df,test_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#adapt this to keras\n",
    "#y_proba = model.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test.shape,y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_flat = np.ndarray.flatten(Y_test)\n",
    "y_pred_flat = np.ndarray.flatten(y_pred)\n",
    "Y_test_flat.shape,y_pred_flat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_flat.dtype,y_pred_flat.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [0.50,0.9,0.99]\n",
    "for thres in thresholds:\n",
    "    print(\"Threshold = \"+str(thres))\n",
    "    print(classification_report(Y_test_flat, y_pred_flat>thres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(Y_test_flat, y_pred_flat) #default prob is already a proba\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "print('Unet model')\n",
    "print('AUC: ' + str(roc_auc))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(fpr, tpr, color = 'brown')\n",
    "plt.plot([0,1],[0,1], '-.', color = 'grey')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('U-Net ROC');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_ind = [0,3,4,6,9,12,15,18,19,20,22,23,25,29,34,36,39,40,48,50,51,52,53,54,55,\\\n",
    "          58,63,64,66,67,69,71,72,73,78,82,86,87,87,88,89,90,92,94,95,97,102,104,\\\n",
    "           108,114,120,121,122,127,129]\n",
    "print(len(bad_ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_tile_no = []\n",
    "for i in bad_ind:\n",
    "    bad_tile_no.append(test_no[i])\n",
    "sorted(bad_tile_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_ex, Y_ex, index = next(example_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_ex[None, ...]) > 0.5   #0.15\n",
    "accuracy = (pred[0,:,:,0]==Y_ex).sum()*1.0/pred.size\n",
    "print(\"image_no:\", index, \"accuracy:\",accuracy)\n",
    "#X_ex.shape,Y_ex.shape,pred[0,:,:,0].shape\n",
    "fixed_X = (92.21524+92.21524*(X_ex[:,:,0:3]+1)).astype('uint8')\n",
    "fixed_X_pre = (92.36813+92.36813*(X_ex[:,:,3:6]+1)).astype('uint8')\n",
    "probmap = model.predict(X_ex[None, ...])[0,:,:,0]\n",
    "\n",
    "fig, (ax1,ax2,ax3) = plt.subplots(1, 3, figsize=(16,10))\n",
    "ax1.set_title(\"Post-Flood Image\")\n",
    "ax2.set_title(\"U-Net Mask\")\n",
    "ax3.set_title(\"Label Mask\")\n",
    "ax1.imshow(fixed_X)\n",
    "ax2.imshow(pred[0,:,:,0])\n",
    "ax3.imshow(Y_ex);\n",
    "\n",
    "fig, (ax1,ax2,ax3) = plt.subplots(1, 3, figsize=(16,10))\n",
    "ax1.set_title(\"Identified Flood Water Only\")\n",
    "ax2.set_title(\"Identified Non-Flood Water Only\")\n",
    "ax3.set_title(\"Raw Model Likelihood\")\n",
    "ax1.imshow(fixed_X*pred[0,:,:,:],)\n",
    "ax2.imshow(fixed_X*(1.-pred[0,:,:,:]).astype('uint8'))\n",
    "ax3.imshow(probmap);\n",
    "\n",
    "fig, (ax1,ax2,ax3) = plt.subplots(1, 3, figsize=(16,10))\n",
    "ax1.set_title(\"Post-Flood Image\")\n",
    "ax2.set_title(\"Identified Non-Flood Water Only\")\n",
    "ax3.set_title(\"Raw Model Likelihood\")\n",
    "ax1.imshow(fixed_X_pre);\n",
    "ax2.imshow(fixed_X*(1.-pred[0,:,:,:]).astype('uint8'))\n",
    "ax3.imshow(probmap);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_ex.sum()/Y_ex.shape[1]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(pred[0,:,:,0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_X = (92.21524+92.21524*(1+X_ex[:,:,0:3])).astype('uint8')\n",
    "plt.imshow(fixed_X);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probmap = model.predict(X_ex[None, ...])[0,:,:,0]\n",
    "print(probmap.shape)\n",
    "plt.imshow(probmap);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets show a few examples.  Each row shows a target class.  Yellow areas are positive, and purple areas are negative.  At this point, you will begin to see that OSM data isn't always accurate.\n",
    "**Top row**: Residential\n",
    "**Middle row**: Forest\n",
    "**Top row**: Water"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_ex, Y_ex = next(example_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_example_class(X_ex, Y_ex, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_ex.shape,Y_ex.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#note from Alan\n",
    "You'll notice that the validation loss is generally decreasing over all the training epochs, and then it eventually goes up a little and then plateaus at 0.0713, with a corresponding validation jaccard coefficient of 0.6682.  This is our final model, although if we want we can go back and grab previous states of the model.  They're all saved in the `checkpoints/` directory.\n",
    "\n",
    "I should issue a word of caution at this point.  I was in a hurry to put this notebook up and I made a few mistakes in my data processing.  I already caught two whoppers that were totally messing up the training, and there might be more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View some output\n",
    "Now I'll test out the model by testing it on a single image and looking at the ground truth against the model prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets show a few examples.  Each row shows a target class.  Yellow areas are positive, and purple areas are negative.  At this point, you will begin to see that OSM data isn't always accurate.\n",
    "**Top row**: Residential\n",
    "**Middle row**: Forest\n",
    "**Top row**: Water"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what would random chance predict?\n",
    "per_flooded = Y_train.sum()/(len(Y_train)*256*256)\n",
    "per_flooded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what would random chance predict?\n",
    "per_flooded = Y_test.sum()/(len(Y_test)*256*256)\n",
    "per_flooded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
